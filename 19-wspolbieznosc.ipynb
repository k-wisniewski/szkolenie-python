{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f9b9b0d-efc1-401d-bb0d-575877c488ef",
   "metadata": {},
   "source": [
    "## Współbieżność i zrównoleglenie\n",
    "Często spotykamy się z sytuacją, gdy w mowie potocznej oba pojęcia używane są wymiennie. Z punktu widzenia informatyki, istnieje fundamentalna różnica między oboma pojęciami: współbieżność jest własnością kodu, natomiast zrównoleglenie dotyczy sposobu konkretnego wykonania. Można zatem myśleć o współbieżności jako o istnieniu \"okazji\" do równoległego wykonania kodu, z której w danej sytuacji można, ale nie trzeba korzystać. Implementacja programu może być więc współbieżna, ale na jednordzeniowym procesorze wykonanie siłą rzeczy równoległe nie będzie - program jednak będzie działał całkowicie poprawnie dzięki systemowi dzielenia czasu.\n",
    "\n",
    "W Pythonie istnieje kilka mechanizmów, które można użyć do stworzenia kodu współbieżnego:\n",
    "- wątki\n",
    "- procesy\n",
    "- generatory, korutyny i programowanie asynchroniczne\n",
    "\n",
    "Poniżej omówimy po kolei różne podejścia do pisania programów współbieżnych w Pythonie.\n",
    "\n",
    "### Wątki\n",
    "Jak wiele innych języków również w Pythonie dostepny jest mechanizm wątków, dzielących wspólną przestrzeń adresową, wspieranych przez system operacyjny (na uniksach z użyciem `pthreads`). Aby stworzyć nowe wątki możemy posłużyć się modułem `threading`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a3fd8f-0262-4a51-9e95-0a1c9003c56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread, get_ident\n",
    "import time\n",
    "\n",
    "def foo():\n",
    "    print(f\"thread with id #{get_ident()} - starting...\")\n",
    "    time.sleep(5)\n",
    "    print(f\"thread: #{get_ident()} - finished!\")\n",
    "\n",
    "threads = []\n",
    "for _ in range(5):\n",
    "    t = Thread(target=foo)\n",
    "    t. start()\n",
    "    threads.append(t)\n",
    "\n",
    "for t in threads:\n",
    "    t.join()\n",
    "    print(f\"main thread: joined thread #{t.ident}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eec128-11c4-49ae-b0ca-0ecfe591b06f",
   "metadata": {},
   "source": [
    "Tak użyte wątki są jednak problematyczne - łatwo zapomnieć o wywołaniu `join()`, co prowadzi do wycieków pamięci. Również oczekiwanie jest nieintuicyjne - nie wiadomo, który wątek skończy się pierwszy, więc nie wiadomo na który czekać w pierszej kolejności. Na szczęście istnieje łatwiejsze rozwiązanie - moduł standardowej biblioteki `concurrent.futures`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2a7f8a5-1bae-4ad0-9922-369b1c57b9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread #0 with id #140698338707008 - starting...  total active: 9\n",
      "thread #1 with id #140698330314304 - starting...  total active: 10\n",
      "thread #2 with id #140698316678720 - starting...  total active: 11\n",
      "thread #2 with id #140698316678720 - finished!    total active: 11\n",
      "thread #3 with id #140698316678720 - starting...  total active: 11\n",
      "thread #1 with id #140698330314304 - finished!    total active: 11\n",
      "thread #4 with id #140698330314304 - starting...  total active: 11\n",
      "thread #0 with id #140698338707008 - finished!    total active: 11\n",
      "thread #4 with id #140698330314304 - finished!    total active: 10\n",
      "thread #3 with id #140698316678720 - finished!    total active: 9\n",
      "finished all - results are in order of increasing threadno: [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "from threading import get_ident\n",
    "import time\n",
    "\n",
    "def foo(threadNo):\n",
    "    print(f\"thread #{threadNo} with id #{get_ident()} - starting...  total active: {threading.active_count()}\")\n",
    "    time.sleep(5 - threadNo)\n",
    "    print(f\"thread #{threadNo} with id #{get_ident()} - finished!    total active: {threading.active_count()}\")\n",
    "    return threadNo\n",
    "    \n",
    "results = None\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    results = executor.map(foo, range(5))\n",
    "print(f\"finished all - results are in order of increasing threadno: {list(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9ffc6f-04e8-4d9f-8b91-b5e7217700b5",
   "metadata": {},
   "source": [
    "Użycie `ThreadPoolExecutor`a jako managera kontekstu jest teraz zalecanym sposobem tworzenia i zarządzania wątkami, który jest stosunkowo odporny na błędy programistyczne powodujące wycieki zasobów. Są jednak pułapki - przykładowo, jeśli do `.map` podamy funkcję bezargumentową, zostanie rzucony wyjątek, który jednak nie wydostanie się poza `ThreadPoolExecutor`, powodując trudne do debugowania bugi.\n",
    "Alternatywnie, możemy użyć innych metod executora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b62eb9e-7009-41fa-b124-f4542d8ea491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import threading\n",
    "def foo(threadNo):\n",
    "    print(f\"thread #{threadNo} with id #{get_ident()} - starting...  total active: {threading.active_count()}\")\n",
    "    time.sleep(5 - threadNo)\n",
    "    print(f\"thread #{threadNo} with id #{get_ident()} - finished!    total active: {threading.active_count()}\")\n",
    "    return threadNo\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    futures = [executor.submit(foo, threadNo) for threadNo in range(5)]\n",
    "    for f in concurrent.futures.as_completed(futures):\n",
    "        print(f\"main: finished {f.result()}\")\n",
    "print(\"finished all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a2f72a-55fc-4a4b-9862-45fdd64760c5",
   "metadata": {},
   "source": [
    "`max_workers` specyfikuje ilość wątków, które będą wykonywać zadania stworzone przy użyciu`executor.submit`. Argumenty `submit` to funkcja implementująca kod zadania i argumenty, które mają być do niej przekazane.\n",
    "\n",
    "### Great Interpreter Lock (GIL) \n",
    "W odróżnieniu od większości innych języków, Python jest dość ograniczony jeśli chodzi o wykorzystanie wątków - na raz wykonuje się tylko jeden z nich, niezależnie od ilości dostępnych rdzeni. Wynika to głównie z gwarancji koniecznych do zapewnienia poprawnego działania garbage collectora - bez muteksa chroniącego liczniki referencji, niemożliwe byłoby zapewnienie bezpieczeństwa zwalniania zasobów. Ograniczenie to dotyczy jednak tylko \"kanonicznej\" implementacji Pythona - CPython, nie istnieje natomiast w PyPy czy IronPythonie. Ponieważ jednak zdecydowana większość programów używa CPythona, ograniczenie to sprawiło, że używanie wątków w rozumieniu biblioteki `threading` czy klasy `ThreadPoolExecutor` zalecane jest tylko dla problemów które są ograniczane przez IO tzw. `IO bound`. W przypadku gdy bottleneckiem jest CPU lepszym rozwiązaniem jest stworzenie osobnych procesów i standardowa komunikacja między nimi w oparciu o gniazda (*sockets*), pamięć dzieloną czy rurki (*pipes*). Wiąże się to jednak ze sporym narzutem, w związku z czym [PEP-703](https://peps.python.org/pep-0703/) postuluje przebudowę CPythona tak, by GIL stał się opcjonalny. Jeśli propozycja zostanie oficjalnie przyjęta - a po raz pierwszy od lat jest na to szansa, wątki staną się pełnoprawnym sposobem implementacji dowolnych programów współbieżnych. Póki co, wątki wykonują się pojedynczo, w związku z czym część programów daje poprawne wyniki, nawet jeśli nie jest do końca poprawna w klasycznym rozumieniu współbieżnośći:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb9997be-8362-4c75-8da8-379ed57006e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 took 1.0267287790047703\n",
      "got 10000000\n",
      "0 took 1.3559310490090866\n",
      "got 10000000\n",
      "2 took 1.4640954999922542\n",
      "4 took 1.4282812790042954\n",
      "3 took 1.4720998650009278\n",
      "got 10000000\n",
      "got 10000000\n",
      "got 10000000\n",
      "5 took 1.3230580749950605\n",
      "got 10000000\n",
      "9 took 1.0640262730012182\n",
      "got 10000000\n",
      "8 took 1.197377423013677\n",
      "got 10000000\n",
      "7 took 1.2486669439967955\n",
      "got 10000000\n",
      "6 took 1.4968561370042153\n",
      "got 10000000\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "\n",
    "def bar(threadNo) -> int:\n",
    "    start = time.monotonic()\n",
    "    bla = 0\n",
    "    for _ in range(10_000_000):\n",
    "        bla += 1\n",
    "    print(f\"{threadNo} took {time.monotonic() - start}\")\n",
    "    return bla\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    futures = [executor.submit(bar, threadNo) for threadNo in range(10)]\n",
    "    for future in as_completed(futures):\n",
    "        print(f\"got {future.result()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff68cb5-78d9-4493-8fd7-7390fc21b0f1",
   "metadata": {},
   "source": [
    "### Interfejs concurrent.Futures.Executor\n",
    "`concurrent.futures` definiuje interfejs `Executor`, wyglądający mniej więcej tak:\n",
    "```\n",
    "class Executor:\n",
    "    def submit(fn, *args, **kwargs):\n",
    "        ...\n",
    "\n",
    "    def map(func, *iterables, timeout=None, chunksize=1):\n",
    "        ...\n",
    "\n",
    "    def shutdown(wait=True, *, cancel_futures=False):\n",
    "        ...\n",
    "```\n",
    "\n",
    "`submit` - zleca wykonanie na którymś z wątków-workerów funkcji `fn` z przekazanymi argumentami. Zwraca obiekt `Future`\n",
    "`map` - uruchamia funkcję `func` na każdym elemencie  kolekcji przekazanych w `iterables`. `chunksize` reguluje jak duże segmenty `iterables` zostają za jednym razem przekazane do workerów - ma to znaczenie zwłaszcza dla implementacji opartej o procesy, gdzie komunikacja międzyprocesowa może być relatywnie dość wolna. Gdy podamy argument `timeout`, podczas iterowania z użyciem zwracanego tu iteratora po upływie tego czasu `next()` spowoduje rzucenie wyjątku `TimeoutError`\n",
    "`shutdown` - sygnalizuje do executora, że należy zwolnić zasoby przez niego zaalokowane. Parametry regulują co dzieje się z niewykonanymi jeszcze obiektami `Future`.\n",
    "\n",
    "Executory implementowane przez bibliotekę `concurrent.futures` są też `contextmanager`ami - nie musimy wołać na nich `shutdown`, bo automatycznie stanie się to jeśli stworzymy executor w bloku `with`:\n",
    "\n",
    "```\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    ...\n",
    "# tu już jest po zawołaniu shutdown\n",
    "```\n",
    "\n",
    "Razem z biblioteką dostajemy 2 implementacje: znany nam już `ThreadPoolExecutor`, który zlecane zadania wykonuje na `max_workers` wątkach, oraz `ProcessPoolExecutor`, osiągający to samo przy użyciu procesów. Sprawia to, że stosunkowo łatwo jest wymienić niewłaściwie dobraną implementację - jeśli po sprofilowaniu okaże się np.: że ograniczeniem w naszym programie jest IO, możemy się pokusić o użycie `ThreadPoolExecutora`, który ma niższy narzut na stworzenie workerów i jest elastyczniejszy w komunikacji między workerami. Dla odmiany procesy ograniczane przez CPU będą znacząco lepiej wykorzystywały dostepne rdzenie procesora.\n",
    "\n",
    "### Procesy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2414a9-b15f-4993-824c-22b67235e922",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "\n",
    "@contextmanager\n",
    "def timer(name: str):\n",
    "    start = time.monotonic()\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        print(f\"{name} took {time.monotonic() - start}\")\n",
    "\n",
    "def bar(threadNo) -> int:\n",
    "    with timer(threadNo):\n",
    "        bla = 0\n",
    "        for _ in range(10_000_000):\n",
    "            bla += 1\n",
    "        return bla\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=5) as executor:\n",
    "    futures = [executor.submit(bar, threadNo) for threadNo in range(10)]\n",
    "    for future in as_completed(futures):\n",
    "        print(f\"got {future.result()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc951f25-d0b9-4c6e-96ee-aaa1295fd9c1",
   "metadata": {},
   "source": [
    "Oczywiście, analogicznie do wątków, istnieje również [niskopoziomowe API](https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing) do zarządzania procesami - służy do tego moduł `multiprocessing`, którego API jest bardzo podobne do `threading`. W wielu przypadkach jest ono jednak trudniejsze do użycia niż `concurrent.futures`, dlatego jeśli można, zaleca się by używać nowszego modułu `concurrent.futures`.\n",
    "\n",
    "### Wymiana danych między procesami\n",
    "Najprostsze w użyciu są 2 udostępniane przez bibliotekę multiprocessing sposoby wymiany danych między procesami: `Queue` i `Pipe`. `Queue` są thread- i process safe, pipe nie.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffea07e8-e9c3-4447-a2e9-093ca9e72410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42, None, 'hello']\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process, Queue\n",
    "\n",
    "def f(q):\n",
    "    q.put([42, None, 'hello']) # <- wysyłamy dane\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    q = Queue() # jedna współdzielona kolejka - muszę ją podać do funkcji implementującej ciało drugiego procesu\n",
    "    p = Process(target=f, args=(q,))\n",
    "    p.start()\n",
    "    print(q.get()) # <- odbieramy dane\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77e1db89-3555-4b82-a0ae-31617be33edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42, None, 'hello']\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process, Pipe\n",
    "\n",
    "def f(conn):\n",
    "    conn.send([42, None, 'hello']) # <- wysyłamy dane\n",
    "    conn.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parent_conn, child_conn = Pipe() # dwie końcówki rurki - dla każdego procesu po 1\n",
    "    p = Process(target=f, args=(child_conn,)) # przekazanie drugiego końca rurki\n",
    "    p.start()\n",
    "    print(parent_conn.recv()) # <- odbieramy dane\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a659337f-e318-4214-927d-d18e5d851642",
   "metadata": {},
   "source": [
    "Można również używać pamięci dzielonej, która jest reprezentowana przy użyciu typów `Value` i `Array` - jeśli jednak nie ma takiej konieczności, zazwyczaj lepszym wyborem będzie `Queue`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefeac94-8420-44f9-8f63-b4a7d99b39d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Value, Array\n",
    "\n",
    "def f(n, a):\n",
    "    n.value = 3.1415927\n",
    "    for i in range(len(a)):\n",
    "        a[i] = -a[i]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    num = Value('d', 0.0)\n",
    "    arr = Array('i', range(10))\n",
    "\n",
    "    p = Process(target=f, args=(num, arr))\n",
    "    p.start()\n",
    "    p.join()\n",
    "\n",
    "    print(num.value)\n",
    "    print(arr[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120c3551-d304-40d7-8ab8-41aaefc6f0e8",
   "metadata": {},
   "source": [
    "### Obiekty Future\n",
    "Wynikiem zasubmitowania funkcji do wykonania na executorze - niezależnie od wybranej implementacji - jest obiekt klasy Future, który enkapsuluje asynchroniczne jej wykonanie. Interfejs tej klasy wygląda mniej więcej tak:\n",
    "```\n",
    "class Future:\n",
    "    def cancel() -> bool:\n",
    "        ...\n",
    "\n",
    "    def cancelled() -> bool:\n",
    "        ...\n",
    "\n",
    "    def running() -> bool:\n",
    "        ...\n",
    "\n",
    "    def done() -> bool:\n",
    "        ...\n",
    "\n",
    "    def result(timeout: int =None) -> Any:\n",
    "        ...\n",
    "\n",
    "    def exception(timeout=None) -> Exception:\n",
    "        ...\n",
    "\n",
    "    def add_done_callback(fn):\n",
    "        ...\n",
    "```\n",
    "Zazwyczaj jednak korzysta się po prostu z funkcji `as_completed` by iterować po kolejnych wynikach kończących sie asynchronicznych obliczeń:\n",
    "```\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    futures = [executor.submit(foo, threadNo) for threadNo in range(5)]\n",
    "    for f in concurrent.futures.as_completed(futures):\n",
    "        ...\n",
    "```\n",
    "\n",
    "### Semafory i zmienne warunkowe\n",
    "Najprostszym typem semafora jest oczywiście mutex, który w Pythonie nazywa się `Lock`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e0272b-4a7c-49b6-8073-25fb9a6c3603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Lock\n",
    "\n",
    "def f(l, i):\n",
    "    l.acquire()\n",
    "    try:\n",
    "        print('hello world', i)\n",
    "    finally:\n",
    "        l.release()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    lock = Lock()\n",
    "\n",
    "    for num in range(10):\n",
    "        Process(target=f, args=(lock, num)).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0b90b0-d967-490c-9606-4b6cab2e0725",
   "metadata": {},
   "source": [
    "... są również generalniejsze semafory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1438b2-b6d0-4cb3-987d-ab019aa78587",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_psql_conns = 5\n",
    "...\n",
    "with BoundedSemaphore(value=max_psql_conns):\n",
    "    conn = connectdb()\n",
    "    try:\n",
    "        # ... use connection ...\n",
    "    finally:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb43e9b0-feb3-4daf-94dc-590490ff5d8c",
   "metadata": {},
   "source": [
    "... i zmienne warunkowe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41899c4f-6551-49f7-a4f5-5b8c401c19a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consumer(cond):\n",
    "    with cond:\n",
    "        while some_queue.isEmpty():\n",
    "            cond.wait()\n",
    "    print('Resource acquired')\n",
    "\n",
    "\n",
    "def producer(cond):\n",
    "    print('Making resource available')\n",
    "    cond.notifyAll()\n",
    "\n",
    "condition = threading.Condition()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af27d5e-29b8-4039-a503-28f151a296cb",
   "metadata": {},
   "source": [
    "### *Zadanie 1*\n",
    "Poniżej podany jest algorytm \"sita Eratostenesa\". Przy użyciu `concurrent.futures` dla każdego `0 < n <= 1 000 000` sprawdź ile jest liczb pierwszych mniejszych od danego `n`. Załóż na potrzeby zadania, że z jakiegoś powodu musisz wykonywać algorytm dla każdej liczby od nowa.\n",
    "* [ ] napisz `contextmanager`, który zmierzy czas obliczenia wszystkich liczb spełniających warunki zadania\n",
    "* [ ] użyj `concurrent.futures` by rozdysponować pracę między wątki\n",
    "* [ ] użyj `concurrent.futures` by rozdysponować pracę między procesy - jaka jest różnica w szybkości programu?\n",
    "* [ ] zainstaluj `pstree` i zweryfikuj ile procesów lub wątków działa w trakcie wykonania Twojego programu\n",
    "* [ ] jak zmienia się czas wykonania w zależności od ilości wątków/procesów podanych jako parametr `max_workers` executora?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "828e4c39-b988-450a-8c74-a48507c10dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 5]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sieve(n):\n",
    "    prime = [True for i in range(n + 1)]\n",
    "    p = 2\n",
    "    while (p * p <= n):\n",
    "        if (prime[p] == True):\n",
    "            for i in range(p * p, n + 1, p):\n",
    "                prime[i] = False\n",
    "        p += 1\n",
    "    result = []\n",
    "    for p in range(2, n + 1):\n",
    "        if prime[p]:\n",
    "            result.append(p)\n",
    "    return len(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb20d65-d4a1-4939-b997-27d28bd7baa8",
   "metadata": {},
   "source": [
    "## Async/await\n",
    "Oprócz \"tradycyjnych\" form współbieżności i równoległości wykonania opartych o wątki i procesy, Python3 wprowadził wsparcie dla korutyn i programowania asynchronicznego. Korutyny to funkcje, które na czas trwania pewnej wolnej operacji - zazwyczaj IO - mogą zostać zawieszone, a gdy operacja się zakonczy ponownie wznowione. Historycznie podobną funkcjonalność oferowały generatory i pierwsze podejście do tego tematu w Pythonie opierało się na nich, jednak było ono dośc niewygodne i nieintuicyjne. Podobnie jak w C# i Node.js, od Pythona 3.7 istnieje możliwość definiowania korutyn przy użyciu słów kluczowych `async` i `await`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2a0b3fe3-c2bb-4afb-aa5a-2c24253eae4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fun2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<coroutine object fun2 at 0x7ff6fc3fedc0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fun(x): # zwykła funkcja\n",
    "    return x * x\n",
    "\n",
    "fun(5) # zwykłe zawołanie zwykłej funkcji\n",
    "\n",
    "async def fun2(x): # asynchroniczna korutyna\n",
    "    print(\"fun2\")\n",
    "    return x * x\n",
    "\n",
    "await fun2(5) # zawołanie korutyny\n",
    "\n",
    "fun2(6) # zwraca obiekt reprezentujący korutynę która jeszcze nie wykonuje się!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3cfdd3-7b90-45f6-8751-bf6653360a9b",
   "metadata": {},
   "source": [
    "Normalnie `await` można użyć tylko wewnątrz korutyny - w przypadku użycia go w zwykłej funkcji dostaniemy błąd:\n",
    "```\n",
    "SyntaxError: 'await' outside async function\n",
    "```\n",
    "Jak w takim razie możemy uruchomić naszą asynchroniczną funkcję w bloku `if __name__ == '__main__'? Musimy stworzyć event loop - przy użyciu modułu `asyncio`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54c173a-8268-4d97-9cc0-d6820164a0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def fun(x):\n",
    "    return x * x\n",
    "\n",
    "\n",
    "async def main() -> None:\n",
    "    result = await fun(10)\n",
    "    print(f'fun(10)={result}')\n",
    "\n",
    "    result = await fun(5)\n",
    "    print(f'fun(5)={result}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39caf43-ea6b-4906-ac08-e2dd208b2c48",
   "metadata": {},
   "source": [
    "W Jupyterze nie powyższy kod nie zadziała - Jupyter wewnątrz sam zaimplementowany jest przy użyciu `asyncio`, więc Python nie pozwala na zagnieżdżone stworzenie nowego event loopa - dlatego poprzedni przykład działal :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
